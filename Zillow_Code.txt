import requests
import pandas as pd
import os
from random import choice
from lxml.html import fromstring
from urllib.parse import urljoin
from bs4 import BeautifulSoup as BS
from fake_useragent import UserAgent, FakeUserAgentError

try:
    ua = UserAgent()
    ua = ua.random
except FakeUserAgentError:
    ua = 'Mozilla/5.0'


def get_proxies():
    url = 'https://free-proxy-list.net/'
    response = requests.get(url, data={'#proxylisttable_length > label > select': '80'})
    parser = fromstring(response.text)
    proxies = set()
    for i in parser.xpath('//tbody/tr'):
        if i.xpath('.//td[7][contains(text(),"yes")]'):
            proxy = ":".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])
            proxies.add(proxy)
    return proxies

proxies = get_proxies()
df = pd.DataFrame(data=list(proxies), columns=['IP_PORT'])
df.to_csv('ip_list.csv')
print(proxies)



try:

    file = os.path.join('ip_list.csv')
except FileNotFoundError:
    raise FileNotFoundError("File not found")

df = pd.read_csv(file)
ips = df.IP_PORT.values.tolist()
ip = choice(ips)


BASE_URL = 'https://www.zillow.com/home/for_sale'
headers = {'user-agent': ua}

proxies = {
    "http": 'http://%s' % ip,
    "https": 'http://%s' % ip
}
res = requests.get('https://www.zillow.com/browse/homes/ca/', headers=headers, proxies=proxies)

soup = BS(res.content, 'lxml')

country = soup.find('div', attrs={'class':'zsg-g browse-content'})

lis = country.select('ul > li')

links = []
for li in lis:
    print(li)
    s = li.find('a', href=True)
    link = s['href'].split('/')[-2]
    print(link)
    link = BASE_URL + link
    print(BASE_URL + link)
    links.append(link)
